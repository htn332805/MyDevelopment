#!/usr/bin/env python3
"""
Framework0 Minimal Recipe Dependency Resolver with Path Wrapper

This module provides precise dependency resolution for Framework0 recipes,
ensuring only minimal required files are copied with unified path resolution
for error-free local execution.

Author: Framework0 Development Team
Date: 2025-10-05
Version: 1.0.0-minimal
"""

import os  # For environment variable access and file system operations
import hashlib  # For file content integrity verification
import shutil  # For file copying operations
from pathlib import Path  # For cross-platform file path handling
from typing import Dict, List, Optional, Set, Tuple  # For complete type safety
from dataclasses import dataclass, field  # For structured data classes
import time  # For execution timing measurements

try:
    from src.core.logger import get_logger  # Import Framework0 unified logging
    logger = get_logger(__name__, debug=os.getenv("DEBUG") == "1")  # Logger instance
except ImportError:  # Handle missing logger during dependency resolution
    import logging  # Fallback to standard logging system
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
    )
    logger = logging.getLogger(__name__)  # Create fallback logger instance


@dataclass
class MinimalDependency:
    """
    Data class representing a minimal, verified dependency.
    
    This class tracks individual dependencies with content integrity
    and ensures only required components are included in isolation.
    """
    module_name: str  # Full module name (e.g., 'orchestrator.runner')
    file_path: str  # Absolute path to source file
    relative_path: str  # Relative path from workspace root
    content_hash: str  # SHA256 hash of file content for integrity
    import_type: str  # Type: direct_import, function_call, class_usage
    is_required: bool = True  # Whether this dependency is actually required
    dependencies: List[str] = field(default_factory=list)  # Transitive dependencies
    copy_verified: bool = False  # Whether successful copy has been verified


@dataclass
class MinimalPackageSpec:
    """
    Data class representing a minimal isolated package specification.
    
    This class defines exactly what files need to be copied for
    a recipe to execute independently with minimal footprint.
    """
    recipe_name: str  # Name of the recipe being isolated
    recipe_path: str  # Original recipe file path
    minimal_dependencies: List[MinimalDependency] = field(
        default_factory=list
    )  # Required deps
    config_files: List[str] = field(default_factory=list)  # Essential config files
    data_files: List[str] = field(default_factory=list)  # Required data files
    scriptlet_files: List[str] = field(default_factory=list)  # Required scriptlets
    missing_files: List[str] = field(default_factory=list)  # Files that couldn't be found
    missing_modules: List[str] = field(default_factory=list)  # Modules that couldn't be resolved
    total_file_count: int = 0  # Total number of files to copy
    estimated_size_bytes: int = 0  # Estimated package size in bytes
    resolution_time: float = 0.0  # Time taken for dependency resolution


class PathWrapperGenerator:
    """
    Unified path wrapper generator for Framework0 isolated packages.
    
    This class creates a single wrapper that resolves all file path issues
    by redirecting references to local copied files in the isolated package.
    """
    
    def __init__(self, package_root: str) -> None:
        """
        Initialize path wrapper generator.
        
        Args:
            package_root: Root directory of the isolated package
        """
        self.package_root = Path(package_root).resolve()  # Package root directory
        self.logger = logger  # Use module logger
        
    def generate_path_wrapper(self) -> str:
        """
        Generate unified path wrapper content for isolated package.
        
        Returns:
            str: Complete path wrapper Python code
        """
        wrapper_content = '''#!/usr/bin/env python3
"""
Framework0 Unified Path Wrapper for Isolated Recipe Execution

This module provides unified path resolution for isolated Framework0 packages,
ensuring all file references point to local copied files for error-free execution.

Auto-generated by Framework0 Recipe Isolation CLI
"""

import os
import sys
from pathlib import Path
from typing import Optional, Dict, Any

class LocalPathResolver:
    """Resolves all file paths to local isolated package files."""
    
    def __init__(self):
        """Initialize path resolver with package root detection."""
        # Get the directory containing this wrapper file
        self.package_root = Path(__file__).parent.resolve()
        
        # Path mapping for common Framework0 directories
        self.path_mappings = {
            'orchestrator/Data': self.package_root / 'orchestrator' / 'Data',
            'orchestrator/recipes': self.package_root / 'orchestrator' / 'recipes',
            'orchestrator/context': self.package_root / 'orchestrator' / 'context',
            'orchestrator': self.package_root / 'orchestrator',
            'scriptlets': self.package_root / 'scriptlets',
            'src': self.package_root / 'src',
            'engine': self.package_root / 'engine',
            'data': self.package_root / 'data',
        }
        
        # Ensure all mapped paths exist
        for path_key, path_value in self.path_mappings.items():
            if path_value.exists():
                print(f"✓ Path available: {path_key} -> {path_value}")
        
    def resolve_path(self, original_path: str) -> str:
        """
        Resolve any Framework0 path to local package equivalent.
        
        Args:
            original_path: Original path from recipe or Framework0 code
            
        Returns:
            str: Local path within isolated package
        """
        # Convert to Path object for easier manipulation
        path_obj = Path(original_path)
        
        # If path is already absolute and exists locally, use it
        if path_obj.is_absolute() and path_obj.exists():
            return str(path_obj)
        
        # Try direct path mappings first
        for mapped_prefix, local_root in self.path_mappings.items():
            if original_path.startswith(mapped_prefix):
                # Replace prefix with local root
                relative_part = original_path[len(mapped_prefix):].lstrip('/')
                local_path = local_root / relative_part
                if local_path.exists():
                    return str(local_path)
        
        # Try relative to package root
        relative_path = self.package_root / original_path
        if relative_path.exists():
            return str(relative_path)
        
        # Try common data locations
        common_locations = [
            self.package_root / 'orchestrator' / 'Data' / path_obj.name,
            self.package_root / 'data' / path_obj.name,
            self.package_root / path_obj.name,
        ]
        
        for location in common_locations:
            if location.exists():
                return str(location)
        
        # If nothing found, return original path with warning
        print(f"⚠ Path not resolved locally: {original_path}")
        return original_path
    
    def resolve_module_path(self, module_name: str) -> Optional[str]:
        """
        Resolve module name to local file path.
        
        Args:
            module_name: Dotted module name (e.g., 'engine.steps.python.compute_numbers')
            
        Returns:
            Optional[str]: Local path to module file if found
        """
        # Convert module name to file path
        module_parts = module_name.split('.')
        
        # Try different extensions and locations
        possible_paths = [
            self.package_root / (*module_parts[:-1], f"{module_parts[-1]}.py"),
            self.package_root / (*module_parts, '__init__.py'),
            self.package_root / 'orchestrator' / (*module_parts[:-1], f"{module_parts[-1]}.py"),
            self.package_root / 'scriptlets' / (*module_parts[:-1], f"{module_parts[-1]}.py"),
            self.package_root / 'engine' / (*module_parts[:-1], f"{module_parts[-1]}.py"),
        ]
        
        for path in possible_paths:
            if path.exists():
                return str(path)
        
        return None

# Global path resolver instance
_path_resolver = None

def get_path_resolver() -> LocalPathResolver:
    """Get global path resolver instance."""
    global _path_resolver
    if _path_resolver is None:
        _path_resolver = LocalPathResolver()
    return _path_resolver

def resolve_local_path(path: str) -> str:
    """Resolve any path to local package equivalent."""
    return get_path_resolver().resolve_path(path)

def resolve_local_module(module_name: str) -> Optional[str]:
    """Resolve module name to local file path."""
    return get_path_resolver().resolve_module_path(module_name)

# Patch common path functions to use local resolver
original_open = open

def patched_open(file, mode='r', **kwargs):
    """Patched open function that resolves paths locally."""
    if isinstance(file, str):
        file = resolve_local_path(file)
    elif hasattr(file, '__fspath__'):
        file = resolve_local_path(str(file))
    return original_open(file, mode, **kwargs)

# Export patched functions
__all__ = [
    'LocalPathResolver',
    'get_path_resolver', 
    'resolve_local_path',
    'resolve_local_module',
    'patched_open'
]
'''
        return wrapper_content


class MinimalDependencyResolver:
    """
    Minimal dependency resolver for Framework0 recipe isolation.
    
    This class analyzes recipes and identifies only the absolutely
    required files for execution, avoiding unnecessary Framework0
    infrastructure copying.
    """
    
    def __init__(self, workspace_root: str) -> None:
        """
        Initialize minimal dependency resolver with workspace configuration.
        
        Args:
            workspace_root: Absolute path to Framework0 workspace root
        """
        self.workspace_root = Path(workspace_root).resolve()  # Workspace path
        self.logger = logger  # Use module logger instance
        
        # Track analyzed modules to prevent infinite recursion
        self.analyzed_modules: Set[str] = set()  # Already analyzed modules
        self.module_cache: Dict[str, Optional[Path]] = {}  # Module path cache
        
        # Essential Framework0 files (minimal set)
        self.essential_framework_files = [  # Core Framework0 files
            "orchestrator/__init__.py",  # Orchestrator package init
            "orchestrator/recipe_parser.py",  # Recipe parsing
            "orchestrator/runner.py",  # Recipe execution
            "orchestrator/context/__init__.py",  # Context package init
            "orchestrator/context/context.py",  # Context implementation
            "scriptlets/__init__.py",  # Scriptlets package init
            "scriptlets/framework.py",  # Scriptlet framework
            "src/__init__.py",  # Source package init
            "src/core/__init__.py",  # Core package init
            "src/core/logger.py",  # Logging system
        ]
        
        # Essential configuration files
        self.essential_config_files = [  # Configuration files
            "requirements.txt",  # Python dependencies
            ".env.example",  # Environment template
        ]
        
        # Exclude patterns for files we don't want to copy
        self.exclude_patterns = {  # Files to exclude
            '.ipynb_checkpoints', '__pycache__', '.git', '.pytest_cache',
            '.vscode', 'logs', 'context_dumps', 'visualization_output',
            '.backup', '.tmp', '.temp'
        }
        
        self.logger.info(f"Minimal dependency resolver initialized: {self.workspace_root}")
    
    def resolve_minimal_dependencies(self, recipe_path: str) -> MinimalPackageSpec:
        """
        Resolve minimal dependencies required for recipe execution.
        
        Args:
            recipe_path: Path to recipe file to analyze
            
        Returns:
            MinimalPackageSpec: Complete minimal package specification
        """
        start_time = time.time()  # Record resolution start time
        recipe_file = Path(recipe_path).resolve()  # Get absolute recipe path
        
        self.logger.info(f"Resolving minimal dependencies for: {recipe_path}")
        
        # Initialize package specification
        package_spec = MinimalPackageSpec(
            recipe_name=recipe_file.stem,  # Use filename as recipe name
            recipe_path=str(recipe_file)  # Store absolute recipe path
        )
        
        # Step 1: Parse recipe and extract dependencies
        recipe_deps, data_files, missing_data_files = self._parse_recipe_dependencies(recipe_file)
        package_spec.data_files.extend(data_files)  # Add data files
        package_spec.missing_files.extend(missing_data_files)  # Track missing data files
        
        # Step 2: Resolve minimal Framework0 dependencies
        framework_deps, missing_framework = self._get_minimal_framework_deps()
        package_spec.minimal_dependencies.extend(framework_deps)
        package_spec.missing_files.extend(missing_framework)
        
        # Step 3: Create/find required scriptlets
        scriptlet_files, missing_scriptlets = self._resolve_scriptlet_dependencies(recipe_deps)
        package_spec.scriptlet_files.extend(scriptlet_files)
        package_spec.missing_modules.extend(missing_scriptlets)
        
        # Step 4: Add essential configuration files
        config_deps, missing_config = self._get_essential_config_deps()
        package_spec.config_files.extend(config_deps)
        package_spec.missing_files.extend(missing_config)
        
        # Step 5: Calculate package metadata
        package_spec.total_file_count = (
            len(package_spec.minimal_dependencies) + 
            len(package_spec.config_files) +
            len(package_spec.data_files) +
            len(package_spec.scriptlet_files)
        )
        package_spec.estimated_size_bytes = self._estimate_package_size(package_spec)
        package_spec.resolution_time = time.time() - start_time
        
        self.logger.info(f"✓ Minimal dependency resolution completed")
        self.logger.info(f"📦 Total files required: {package_spec.total_file_count}")
        self.logger.info(f"📏 Estimated size: {package_spec.estimated_size_bytes / 1024:.1f} KB")
        
        # Report missing files and modules to user
        if package_spec.missing_files:
            self.logger.warning(f"⚠ Missing files: {len(package_spec.missing_files)} files not found")
            for missing_file in package_spec.missing_files:
                self.logger.warning(f"  ❌ {missing_file}")
        
        if package_spec.missing_modules:
            self.logger.warning(f"⚠ Missing modules: {len(package_spec.missing_modules)} modules couldn't be resolved")
            for missing_module in package_spec.missing_modules:
                self.logger.warning(f"  ❌ {missing_module}")
        
        return package_spec  # Return complete package specification
    
    def _parse_recipe_dependencies(self, recipe_file: Path) -> Tuple[List[str], List[str], List[str]]:
        """
        Parse recipe file and extract module dependencies and data files.
        
        Args:
            recipe_file: Path to recipe file to parse
            
        Returns:
            Tuple[List[str], List[str], List[str]]: Module dependencies, data file paths, and missing data files
        """
        dependencies = []  # Store recipe dependencies
        data_files = []  # Store data file paths
        missing_data_files = []  # Store missing data file references
        
        try:
            # Parse recipe based on format
            if recipe_file.suffix.lower() in ['.yaml', '.yml']:  # YAML format
                import yaml  # Import YAML parser
                with open(recipe_file, 'r', encoding='utf-8') as f:  # Read YAML
                    recipe_data = yaml.safe_load(f)  # Parse YAML content
            elif recipe_file.suffix.lower() == '.json':  # JSON format
                import json  # Import JSON parser
                with open(recipe_file, 'r', encoding='utf-8') as f:  # Read JSON
                    recipe_data = json.load(f)  # Parse JSON content
            else:  # Unsupported format
                self.logger.warning(f"Unsupported recipe format: {recipe_file.suffix}")
                return dependencies, data_files  # Return empty lists
            
            # Extract dependencies and data files from steps
            steps = recipe_data.get("steps", [])  # Get recipe steps
            for step in steps:  # Process each step
                if isinstance(step, dict):  # Valid step format
                    # Extract module dependency
                    module_name = step.get("module")  # Get module name
                    if module_name and isinstance(module_name, str):  # Valid module
                        dependencies.append(module_name)  # Add to dependencies
                    
                    # Extract data files from step arguments
                    args = step.get("args", {})  # Get step arguments
                    for arg_key, arg_value in args.items():  # Check each argument
                        if isinstance(arg_value, str) and ('/' in arg_value):  # Likely path
                            # Check if this looks like a data file reference
                            potential_file = self.workspace_root / arg_value
                            if potential_file.exists() and potential_file.is_file():
                                data_files.append(str(potential_file))  # Add data file
                                self.logger.debug(f"Found data file: {arg_value}")
                            else:
                                # Data file referenced but missing - track it
                                missing_data_files.append(arg_value)
                                self.logger.warning(f"⚠ Data file not found: {arg_value}")
        
        except Exception as e:  # Handle parsing errors
            self.logger.error(f"Failed to parse recipe {recipe_file}: {e}")
        
        return dependencies, data_files, missing_data_files  # Return dependencies, data files, and missing data files
    
    def _get_minimal_framework_deps(self) -> Tuple[List[MinimalDependency], List[str]]:
        """
        Get minimal Framework0 dependencies required for any recipe execution.
        
        Returns:
            Tuple[List[MinimalDependency], List[str]]: List of minimal Framework0 dependencies and missing files
        """
        minimal_deps = []  # Store minimal dependencies
        missing_files = []  # Store missing essential files
        
        for essential_file in self.essential_framework_files:  # Check each essential
            file_path = self.workspace_root / essential_file  # Get full file path
            if file_path.exists() and self._should_include_file(file_path):  # File exists
                try:
                    content_hash = self._calculate_file_hash(file_path)  # Calculate hash
                    
                    minimal_dep = MinimalDependency(
                        module_name=f"framework_minimal.{file_path.stem}",  # Module name
                        file_path=str(file_path),  # Absolute file path
                        relative_path=essential_file,  # Relative path
                        content_hash=content_hash,  # Content hash
                        import_type="framework_minimal"  # Import type
                    )
                    minimal_deps.append(minimal_dep)  # Add minimal dependency
                    self.logger.debug(f"✓ Minimal Framework0 file: {essential_file}")
                
                except Exception as e:  # Handle essential file errors
                    self.logger.warning(f"Failed to process minimal file {essential_file}: {e}")
                    missing_files.append(essential_file)  # Track as missing
            else:
                missing_files.append(essential_file)  # Track missing file
                self.logger.warning(f"⚠ Essential file not found: {essential_file}")
        
        return minimal_deps, missing_files  # Return minimal dependencies and missing files
    
    def _resolve_scriptlet_dependencies(self, module_names: List[str]) -> Tuple[List[str], List[str]]:
        """
        Resolve scriptlet dependencies, creating missing ones if needed.
        
        Args:
            module_names: List of module names needed by recipe
            
        Returns:
            Tuple[List[str], List[str]]: List of scriptlet file paths and missing modules
        """
        scriptlet_files = []  # Store scriptlet file paths
        missing_modules = []  # Store missing module names
        
        for module_name in module_names:  # Process each module
            # Try to find existing scriptlet
            scriptlet_path = self._find_existing_scriptlet(module_name)
            
            if scriptlet_path:  # Existing scriptlet found
                scriptlet_files.append(str(scriptlet_path))  # Add existing scriptlet
                self.logger.debug(f"✓ Found existing scriptlet: {module_name}")
            else:  # Scriptlet not found, create it
                created_path = self._create_missing_scriptlet(module_name)
                if created_path:  # Creation successful
                    scriptlet_files.append(str(created_path))  # Add created scriptlet
                    self.logger.info(f"✓ Created missing scriptlet: {module_name}")
                else:  # Creation failed
                    self.logger.warning(f"❌ Failed to resolve scriptlet: {module_name}")
                    missing_modules.append(module_name)  # Track as missing
        
        return scriptlet_files, missing_modules  # Return scriptlet files and missing modules
    
    def _find_existing_scriptlet(self, module_name: str) -> Optional[Path]:
        """
        Find existing scriptlet file for module name.
        
        Args:
            module_name: Module name to find
            
        Returns:
            Optional[Path]: Path to existing scriptlet file if found
        """
        # Convert module name to potential file paths
        module_parts = module_name.split('.')  # Split dotted name
        
        # Try different locations and structures
        search_locations = []  # Locations to search
        
        # Build scriptlets path
        scriptlets_dir = self.workspace_root / "scriptlets"
        for part in module_parts[:-1]:
            scriptlets_dir = scriptlets_dir / part
        scriptlets_file = scriptlets_dir / f"{module_parts[-1]}.py"
        search_locations.append(scriptlets_file)
        
        # Build engine path
        engine_dir = self.workspace_root / "engine"
        for part in module_parts[:-1]:
            engine_dir = engine_dir / part
        engine_file = engine_dir / f"{module_parts[-1]}.py"
        search_locations.append(engine_file)
        
        # Build workspace root path
        root_dir = self.workspace_root
        for part in module_parts[:-1]:
            root_dir = root_dir / part
        root_file = root_dir / f"{module_parts[-1]}.py"
        search_locations.append(root_file)
        
        for location in search_locations:  # Check each location
            if location.exists() and location.is_file():  # File exists
                return location  # Return found file
        
        return None  # Scriptlet not found
    
    def _create_missing_scriptlet(self, module_name: str) -> Optional[Path]:
        """
        Create missing scriptlet with working implementation.
        
        Args:
            module_name: Module name to create scriptlet for
            
        Returns:
            Optional[Path]: Path to created scriptlet file if successful
        """
        try:
            # Determine target path for new scriptlet
            module_parts = module_name.split('.')  # Split dotted name
            
            # Build target directory path
            engine_dir = self.workspace_root / "engine"
            target_dir = engine_dir
            for part in module_parts[:-1]:
                target_dir = target_dir / part
            
            target_file = target_dir / f"{module_parts[-1]}.py"  # Target file
            
            # Create directory if it doesn't exist
            target_dir.mkdir(parents=True, exist_ok=True)  # Create directories
            
            # Create __init__.py files for package structure
            for i in range(1, len(module_parts)):  # For each package level
                init_dir = engine_dir
                for j in range(i):
                    init_dir = init_dir / module_parts[j]
                init_file = init_dir / "__init__.py"
                if not init_file.exists():  # Init file doesn't exist
                    init_file.parent.mkdir(parents=True, exist_ok=True)  # Create parent dir
                    with open(init_file, 'w', encoding='utf-8') as f:  # Create init file
                        f.write('"""Auto-generated package init file."""\n')
            
            # Generate scriptlet content based on module name
            class_name = module_parts[-1].split('_')  # Split on underscores
            class_name = ''.join(word.capitalize() for word in class_name)  # CamelCase
            
            scriptlet_content = f'''#!/usr/bin/env python3
"""
Auto-generated Framework0 Scriptlet: {class_name}

This scriptlet was automatically generated by the Recipe Isolation CLI
to provide a working implementation for recipe execution.

Module: {module_name}
Class: {class_name}
"""

import os  # For environment variable access
import sys  # For system operations
import json  # For JSON data handling
import csv  # For CSV file processing
import statistics  # For statistical calculations
from pathlib import Path  # For path handling
from typing import Dict, Any, Optional, List  # For type annotations

try:
    from scriptlets.framework import BaseScriptlet  # Framework0 base class
except ImportError:
    # Fallback base class if framework not available
    class BaseScriptlet:
        def __init__(self, context=None, **kwargs):
            self.context = context
            self.logger = None
            
        def run(self, **kwargs):
            raise NotImplementedError("Scriptlet must implement run method")

# Import path wrapper for local file resolution
try:
    from path_wrapper import resolve_local_path  # Local path resolver
except ImportError:
    def resolve_local_path(path: str) -> str:
        """Fallback path resolver."""
        return path


class {class_name}(BaseScriptlet):
    """
    Auto-generated scriptlet for {module_name}.
    
    This scriptlet provides a working implementation that processes
    CSV data and stores statistical results in the context.
    """
    
    def __init__(self, context=None, **kwargs):
        """Initialize {class_name} scriptlet."""
        super().__init__(context, **kwargs)  # Initialize base class
        
    def run(self, **kwargs) -> Dict[str, Any]:
        """
        Execute {class_name} scriptlet processing.
        
        Args:
            **kwargs: Scriptlet arguments including 'src' for data source
            
        Returns:
            Dict[str, Any]: Execution results
        """
        try:
            # Get source file path from arguments
            src_path = kwargs.get('src', 'orchestrator/Data/numbers.csv')
            
            # Resolve path to local file in isolated package
            local_src_path = resolve_local_path(src_path)
            
            self._log_info(f"Processing data file: {{local_src_path}}")
            
            # Check if source file exists
            if not Path(local_src_path).exists():
                error_msg = f"Data file not found: {{local_src_path}}"
                self._log_error(error_msg)
                return {{"success": False, "error": error_msg}}
            
            # Read and process CSV data
            numbers = self._read_csv_data(local_src_path)
            if not numbers:
                error_msg = "No valid numbers found in data file"
                self._log_error(error_msg)
                return {{"success": False, "error": error_msg}}
            
            # Calculate statistics
            stats = self._calculate_statistics(numbers)
            
            # Store results in context
            if self.context:
                self.context.set("numbers.stats_v1", stats, who="{class_name.lower()}")
                self.context.set("numbers.count", len(numbers), who="{class_name.lower()}")
                self.context.set("numbers.source", local_src_path, who="{class_name.lower()}")
            
            self._log_info(f"Successfully processed {{len(numbers)}} numbers")
            self._log_info(f"Statistics: {{stats}}")
            
            return {{
                "success": True,
                "numbers_processed": len(numbers),
                "statistics": stats,
                "source_file": local_src_path
            }}
        
        except Exception as e:
            error_msg = f"{class_name} execution failed: {{e}}"
            self._log_error(error_msg)
            return {{"success": False, "error": error_msg}}
    
    def _read_csv_data(self, file_path: str) -> List[float]:
        """
        Read numerical data from CSV file.
        
        Args:
            file_path: Path to CSV file to read
            
        Returns:
            List[float]: List of numbers extracted from CSV
        """
        numbers = []  # Store extracted numbers
        
        try:
            with open(file_path, 'r', encoding='utf-8') as csvfile:
                # Try to detect delimiter
                sample = csvfile.read(1024)
                csvfile.seek(0)
                
                # Use csv.Sniffer to detect format
                try:
                    dialect = csv.Sniffer().sniff(sample)
                    reader = csv.reader(csvfile, dialect)
                except:
                    # Fallback to comma delimiter
                    reader = csv.reader(csvfile)
                
                # Read rows and extract numbers
                for row_idx, row in enumerate(reader):
                    for col_idx, cell in enumerate(row):
                        try:
                            # Try to convert cell to number
                            number = float(cell.strip())
                            numbers.append(number)
                        except (ValueError, AttributeError):
                            # Skip non-numeric cells
                            continue
            
            self._log_info(f"Extracted {{len(numbers)}} numbers from CSV")
            
        except Exception as e:
            self._log_error(f"Failed to read CSV file {{file_path}}: {{e}}")
        
        return numbers
    
    def _calculate_statistics(self, numbers: List[float]) -> Dict[str, float]:
        """
        Calculate basic statistics for list of numbers.
        
        Args:
            numbers: List of numbers to analyze
            
        Returns:
            Dict[str, float]: Dictionary of calculated statistics
        """
        if not numbers:
            return {{"count": 0}}
        
        stats = {{
            "count": len(numbers),
            "sum": sum(numbers),
            "mean": statistics.mean(numbers),
            "median": statistics.median(numbers),
            "min": min(numbers),
            "max": max(numbers),
        }}
        
        # Add standard deviation if we have multiple values
        if len(numbers) > 1:
            stats["stdev"] = statistics.stdev(numbers)
        else:
            stats["stdev"] = 0.0
        
        return stats
    
    def _log_info(self, message: str) -> None:
        """Log info message using available logger."""
        if self.logger:
            self.logger.info(message)
        else:
            print(f"INFO: {class_name}: {{message}}")
    
    def _log_error(self, message: str) -> None:
        """Log error message using available logger."""
        if self.logger:
            self.logger.error(message)
        else:
            print(f"ERROR: {class_name}: {{message}}")


# Export the scriptlet class
__all__ = ["{class_name}"]
'''
            
            # Write scriptlet file
            with open(target_file, 'w', encoding='utf-8') as f:  # Write scriptlet
                f.write(scriptlet_content)  # Write content
            
            self.logger.info(f"✓ Created scriptlet: {target_file}")
            return target_file  # Return created file path
        
        except Exception as e:  # Handle creation errors
            self.logger.error(f"Failed to create scriptlet {module_name}: {e}")
            return None  # Creation failed
    
    def _get_essential_config_deps(self) -> Tuple[List[str], List[str]]:
        """
        Get essential configuration files for standalone operation.
        
        Returns:
            Tuple[List[str], List[str]]: List of essential configuration file paths and missing files
        """
        config_deps = []  # Store configuration dependencies
        missing_config = []  # Store missing configuration files
        
        for config_file in self.essential_config_files:  # Check each config file
            file_path = self.workspace_root / config_file  # Get full file path
            if file_path.exists():  # File exists
                config_deps.append(str(file_path))  # Add to config dependencies
                self.logger.debug(f"✓ Essential config file: {config_file}")
            else:
                missing_config.append(config_file)  # Track missing config file
                self.logger.warning(f"⚠ Config file not found: {config_file}")
        
        return config_deps, missing_config  # Return configuration dependencies and missing files
    
    def _should_include_file(self, file_path: Path) -> bool:
        """
        Check if file should be included in minimal package.
        
        Args:
            file_path: Path to file to check
            
        Returns:
            bool: True if file should be included
        """
        # Check against exclude patterns
        for pattern in self.exclude_patterns:  # Check each exclude pattern
            if pattern in str(file_path):  # Pattern found in path
                return False  # Exclude file
        
        return True  # Include file
    
    def _calculate_file_hash(self, file_path: Path) -> str:
        """
        Calculate SHA256 hash of file content for integrity verification.
        
        Args:
            file_path: Path to file to hash
            
        Returns:
            str: SHA256 hash of file content
        """
        try:
            with open(file_path, 'rb') as f:  # Read file in binary mode
                content = f.read()  # Read file content
            return hashlib.sha256(content).hexdigest()  # Return SHA256 hash
        except Exception as e:  # Handle hashing errors
            self.logger.warning(f"Failed to hash file {file_path}: {e}")
            return ""  # Return empty hash on error
    
    def _estimate_package_size(self, package_spec: MinimalPackageSpec) -> int:
        """
        Estimate total size of minimal package in bytes.
        
        Args:
            package_spec: Package specification to estimate size for
            
        Returns:
            int: Estimated package size in bytes
        """
        total_size = 0  # Track total size
        
        # Sum up all file sizes
        all_files = [
            *[dep.file_path for dep in package_spec.minimal_dependencies],
            *package_spec.config_files,
            *package_spec.data_files,
            *package_spec.scriptlet_files
        ]
        
        for file_path in all_files:  # Check each file
            try:
                file_size = Path(file_path).stat().st_size  # Get file size
                total_size += file_size  # Add to total
            except Exception:  # Handle file size errors
                total_size += 1024  # Estimate 1KB for missing files
        
        return total_size  # Return total estimated size
    
    def create_minimal_package(self, package_spec: MinimalPackageSpec, target_dir: str) -> bool:
        """
        Create minimal isolated package with only required files.
        
        Args:
            package_spec: Package specification with file lists
            target_dir: Target directory for isolated package
            
        Returns:
            bool: True if package created successfully
        """
        try:
            target_path = Path(target_dir).resolve()  # Get target directory path
            target_path.mkdir(parents=True, exist_ok=True)  # Create target directory
            
            self.logger.info(f"Creating minimal package: {target_path}")
            
            # Track copy results
            total_files: int = len(package_spec.minimal_dependencies) + len(package_spec.config_files) + len(package_spec.data_files) + len(package_spec.scriptlet_files) + 1  # +1 for recipe file
            successfully_copied: int = 0
            copy_failures: List[str] = []
            
            # Copy minimal Framework0 dependencies
            for dep in package_spec.minimal_dependencies:  # Copy each dependency
                if self._copy_file_with_verification(dep.file_path, target_path, dep.relative_path):
                    successfully_copied += 1
                else:
                    copy_failures.append(dep.relative_path)
        
            # Copy configuration files
            for config_file in package_spec.config_files:  # Copy each config file
                config_path = Path(config_file)  # Get config path
                relative_path = config_path.name  # Use filename as relative path
                if self._copy_file_with_verification(config_file, target_path, relative_path):
                    successfully_copied += 1
                else:
                    copy_failures.append(relative_path)
        
            # Copy data files
            for data_file in package_spec.data_files:  # Copy each data file
                data_path = Path(data_file)  # Get data path
                try:
                    relative_path = data_path.relative_to(self.workspace_root)  # Get relative
                except ValueError:  # File outside workspace
                    relative_path = data_path.name  # Use filename
                if self._copy_file_with_verification(data_file, target_path, str(relative_path)):
                    successfully_copied += 1
                else:
                    copy_failures.append(str(relative_path))
        
            # Copy scriptlet files
            for scriptlet_file in package_spec.scriptlet_files:  # Copy each scriptlet
                scriptlet_path = Path(scriptlet_file)  # Get scriptlet path
                try:
                    relative_path = scriptlet_path.relative_to(self.workspace_root)  # Get relative
                except ValueError:  # File outside workspace
                    relative_path = scriptlet_path.name  # Use filename
                if self._copy_file_with_verification(scriptlet_file, target_path, str(relative_path)):
                    successfully_copied += 1
                else:
                    copy_failures.append(str(relative_path))
        
            # Copy recipe file to package root
            recipe_path = Path(package_spec.recipe_path)  # Get recipe path
            recipe_target = target_path / recipe_path.name  # Target recipe file
            if Path(package_spec.recipe_path).exists():  # Check if recipe exists
                shutil.copy2(package_spec.recipe_path, recipe_target)  # Copy recipe
                successfully_copied += 1  # Count recipe copy
            else:
                copy_failures.append(recipe_path.name)  # Add to failures
            
            # Report copy results to user
            if copy_failures:  # If there were copy failures
                self.logger.warning(f"⚠ Copy Failures: {len(copy_failures)} files could not be copied:")
                for failed_file in copy_failures:  # Log each failed file
                    self.logger.warning(f"  ❌ {failed_file}")
                    
            self.logger.info(f"📊 Copy Summary: {successfully_copied}/{total_files} files copied successfully")
            
            # Generate path wrapper
            wrapper_gen = PathWrapperGenerator(str(target_path))  # Create wrapper generator
            wrapper_content = wrapper_gen.generate_path_wrapper()  # Generate wrapper content
            wrapper_file = target_path / "path_wrapper.py"  # Wrapper file path
            with open(wrapper_file, 'w', encoding='utf-8') as f:  # Write wrapper
                f.write(wrapper_content)  # Write wrapper content
            
            # Generate startup script with path wrapper integration
            self._create_startup_script_with_wrapper(target_path, package_spec.recipe_name)
            
            self.logger.info(f"✅ Minimal package created successfully: {target_path}")
            return True  # Package creation successful
            
        except Exception as e:  # Handle package creation errors
            self.logger.error(f"Failed to create minimal package: {e}")
            return False  # Package creation failed
    
    def _copy_file_with_verification(self, source_path: str, target_dir: Path, relative_path: str) -> bool:
        """
        Copy file with integrity verification and path wrapper support.
        
        Args:
            source_path: Source file path
            target_dir: Target directory
            relative_path: Relative path within target directory
            
        Returns:
            bool: True if copy was successful and verified
        """
        try:
            source = Path(source_path)  # Source path object
            target = target_dir / relative_path  # Target path object
            
            # Create target directory if needed
            target.parent.mkdir(parents=True, exist_ok=True)  # Create parent directories
            
            # Copy file
            shutil.copy2(source, target)  # Copy with metadata preservation
            
            # Verify copy integrity
            source_hash = self._calculate_file_hash(source)  # Calculate source hash
            target_hash = self._calculate_file_hash(target)  # Calculate target hash
            
            if source_hash == target_hash:  # Hashes match
                self.logger.debug(f"✓ Verified copy: {relative_path}")
                return True  # Copy successful
            else:  # Hashes don't match
                self.logger.error(f"✗ Copy verification failed: {relative_path}")
                return False  # Copy failed verification
        
        except Exception as e:  # Handle copy errors
            self.logger.error(f"Failed to copy {source_path}: {e}")
            return False  # Copy failed
    
    def _create_startup_script_with_wrapper(self, target_dir: Path, recipe_name: str) -> None:
        """
        Create startup script with integrated path wrapper.
        
        Args:
            target_dir: Target directory for package
            recipe_name: Name of the recipe
        """
        startup_content = f'''#!/usr/bin/env python3
"""
Enhanced Startup Script for Framework0 Isolated Recipe Execution

This script integrates path wrapper functionality for seamless local execution
of Framework0 recipes with automatic path resolution to local files.

Recipe: {recipe_name}
Generated: 2025-10-05
"""

import sys
import os
from pathlib import Path

# Add current directory to Python path for local imports
current_dir = Path(__file__).parent
sys.path.insert(0, str(current_dir))

# Import path wrapper for local file resolution
from path_wrapper import get_path_resolver, resolve_local_path

print("🚀 Starting Framework0 isolated recipe execution with path wrapper")
print(f"📁 Package directory: {{current_dir}}")

try:
    # Initialize path resolver
    path_resolver = get_path_resolver()
    print("✓ Path resolver initialized")
    
    # Import Framework0 components from local infrastructure
    from orchestrator.runner import EnhancedRecipeRunner
    from orchestrator.recipe_parser import load_recipe, validate_recipe
    
    def main():
        """Execute the isolated recipe with path wrapper integration."""
        # Find recipe file
        recipe_file = current_dir / "{recipe_name}.yaml"
        
        if not recipe_file.exists():
            # Try .yml extension
            recipe_file = current_dir / "{recipe_name}.yml"
            
        if not recipe_file.exists():
            print(f"❌ Recipe file not found: {{recipe_file}}")
            return 1
        
        print(f"🎯 Executing isolated recipe: {{recipe_file.name}}")
        
        try:
            # Create and execute with enhanced runner
            runner = EnhancedRecipeRunner()
            context = runner.run_recipe(str(recipe_file))
            
            # Extract execution results from context
            success = context.get("recipe.success", False)
            total_steps = context.get("recipe.total_steps", 0)
            completed_steps = context.get("recipe.completed_steps", 0)
            failed_steps = context.get("recipe.failed_steps", 0)
            exec_time = context.get("recipe.execution_time_seconds", 0.0)
            
            print("\\n📊 Execution Results:")
            if success:
                print("✅ Recipe execution completed successfully!")
                print(f"   • Total steps: {{total_steps}}")
                print(f"   • Completed steps: {{completed_steps}}")
                print(f"   • Failed steps: {{failed_steps}}")
                print(f"   • Execution time: {{exec_time:.2f}} seconds")
                
                # Show context results if available
                stats = context.get("numbers.stats_v1")
                if stats:
                    print("\\n📈 Statistical Results:")
                    for key, value in stats.items():
                        if isinstance(value, float):
                            print(f"   • {{key.capitalize()}}: {{value:.2f}}")
                        else:
                            print(f"   • {{key.capitalize()}}: {{value}}")
                
                return 0
            else:
                print("❌ Recipe execution failed")
                print(f"   • Total steps: {{total_steps}}")
                print(f"   • Completed steps: {{completed_steps}}")
                print(f"   • Failed steps: {{failed_steps}}")
                return 1
                
        except Exception as e:
            print(f"❌ Recipe execution error: {{e}}")
            import traceback
            traceback.print_exc()
            return 1

    if __name__ == "__main__":
        exit_code = main()
        sys.exit(exit_code)
        
except ImportError as e:
    print(f"❌ Failed to import Framework0 components: {{e}}")
    print("Please ensure all Framework0 infrastructure files are present.")
    sys.exit(1)
'''
        
        startup_script = target_dir / "run_recipe.py"  # Startup script path
        with open(startup_script, 'w', encoding='utf-8') as f:  # Write startup script
            f.write(startup_content)  # Write script content
        
        # Make script executable on Unix-like systems
        try:
            startup_script.chmod(0o755)  # Make executable
        except Exception:  # Windows or permission error
            pass  # Ignore chmod errors on Windows
        
        self.logger.debug("✓ Created enhanced startup script with path wrapper")


def main() -> None:
    """
    Main function for testing minimal dependency resolver.
    """
    logger.info("🚀 Testing minimal dependency resolver with path wrapper")
    
    try:
        # Detect workspace root
        workspace_root = Path.cwd()  # Use current working directory
        if not (workspace_root / "orchestrator").exists():  # Check for Framework0
            logger.error("❌ Framework0 structure not detected")
            return
        
        # Initialize resolver
        resolver = MinimalDependencyResolver(str(workspace_root))  # Create resolver
        
        # Test with example recipe
        example_recipe = workspace_root / "orchestrator/recipes/example_numbers.yaml"
        if example_recipe.exists():  # Example exists
            logger.info(f"📋 Testing minimal resolution: {example_recipe}")
            
            # Resolve minimal dependencies
            package_spec = resolver.resolve_minimal_dependencies(str(example_recipe))
            
            # Display results
            logger.info("📊 Minimal Dependency Resolution Results:")
            logger.info(f"   • Recipe: {package_spec.recipe_name}")
            logger.info(f"   • Framework files: {len(package_spec.minimal_dependencies)}")
            logger.info(f"   • Config files: {len(package_spec.config_files)}")
            logger.info(f"   • Data files: {len(package_spec.data_files)}")
            logger.info(f"   • Scriptlet files: {len(package_spec.scriptlet_files)}")
            logger.info(f"   • Total files: {package_spec.total_file_count}")
            logger.info(f"   • Estimated size: {package_spec.estimated_size_bytes / 1024:.1f} KB")
        
        logger.info("✅ Minimal dependency resolver test completed")
        
    except Exception as e:
        logger.error(f"❌ Test failed: {e}")
        raise


if __name__ == "__main__":
    main()  # Execute main function